{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: opencv-contrib-python in c:\\users\\wonseyoung\\appdata\\roaming\\python\\python38\\site-packages (4.5.1.48)\n",
      "Requirement already satisfied: numpy>=1.17.3 in c:\\users\\wonseyoung\\anaconda3\\lib\\site-packages (from opencv-contrib-python) (1.19.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install opencv-contrib-python --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install opencv-contrib-python --user\n",
    "\n",
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "import pdb\n",
    "\n",
    "def face_recog_train():\n",
    "    dataset_path = 'dataset/'\n",
    "    dirs = os.listdir(dataset_path)#dataset 하위 디렉토리 이름을 리스트에 저장\n",
    "    persons = []\n",
    "    for dir in dirs: #각 디렉토리에 저장된 파일명들을 persons에 담음\n",
    "        if os.path.isdir(dataset_path+dir):\n",
    "            files = os.listdir(dataset_path+dir)\n",
    "            for idx, f in enumerate(files):\n",
    "                files[idx] = dataset_path+dir+'/'+f\n",
    "            persons.append(files)\n",
    "    #print(persons)\n",
    "    \n",
    "    classifiers = cv2.CascadeClassifier('haar/haarcascade_frontalface_alt2.xml') # 얼굴 분류기\n",
    "    recognizer = cv2.face.LBPHFaceRecognizer_create() # 얼굴 인식기\n",
    "    samples = []\n",
    "    ids = []\n",
    "    #학습할 얼굴 샘플링\n",
    "    for id, row in enumerate(persons): # id:0, 정우성 사진들 / id:1. 이나영 사진들\n",
    "        for p in row:\n",
    "            img = cv2.imread(p)\n",
    "            print(p)\n",
    "            gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "            face = classifiers.detectMultiScale(\n",
    "                gray,\n",
    "                scaleFactor=1.2,\n",
    "                minNeighbors=5,\n",
    "                minSize=(20, 20)\n",
    "            )\n",
    "            for (x,y,w,h) in face:\n",
    "                samples.append(gray[y:y+h, x:x+w])\n",
    "                ids.append(id)\n",
    "\n",
    "    recognizer.train(samples, np.array(ids))\n",
    "    recognizer.write('trainer.yml')\n",
    "    print('얼굴 학습 완료')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'cv2.cv2' has no attribute 'face'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-28-cbf1353dcb96>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mface_recog_train\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-27-37cb1dd5865c>\u001b[0m in \u001b[0;36mface_recog_train\u001b[1;34m()\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[0mclassifiers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCascadeClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'haar/haarcascade_frontalface_alt2.xml'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# 얼굴 분류기\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m     \u001b[0mrecognizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mface\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLBPHFaceRecognizer_create\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# 얼굴 인식기\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m     \u001b[0msamples\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[0mids\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'cv2.cv2' has no attribute 'face'"
     ]
    }
   ],
   "source": [
    "face_recog_train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def face_recog():\n",
    "\n",
    "    classifiers = cv2.CascadeClassifier('haar/haarcascade_frontalface_alt2.xml')\n",
    "    recognizer = cv2.face.LBPHFaceRecognizer_create()\n",
    "    \n",
    "    recognizer.read('trainer.yml')\n",
    "\n",
    "    names=['정우성', '이나영']\n",
    "    img = cv2.imread('dataset/p1/AnyConv.com__4.jpg')\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    face = classifiers.detectMultiScale(\n",
    "        gray,\n",
    "        scaleFactor=1.2,\n",
    "        minNeighbors=5,\n",
    "        minSize=(20, 20)\n",
    "    )\n",
    "    res = False\n",
    "    name = 'no face'\n",
    "    for (x, y, w, h) in face:\n",
    "        id, confi = recognizer.predict(gray[y:y+h, x:x+w])\n",
    "        if confi < 50:\n",
    "            name = names[id]\n",
    "            print(name,'/ confidence:', confi)\n",
    "            res = True\n",
    "        else:\n",
    "            name = 'unknown'\n",
    "            res = False\n",
    "\n",
    "    return res, name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'cv2.cv2' has no attribute 'face'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-5d7af6706d4e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mface_recog\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-10-4065219052b5>\u001b[0m in \u001b[0;36mface_recog\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mclassifiers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCascadeClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'haar/haarcascade_frontalface_alt2.xml'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mrecognizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mface\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLBPHFaceRecognizer_create\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mrecognizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'trainer.yml'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'cv2.cv2' has no attribute 'face'"
     ]
    }
   ],
   "source": [
    "face_recog()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
